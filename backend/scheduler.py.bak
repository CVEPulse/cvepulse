# backend/scheduler.py
import os, json, math, requests
from datetime import datetime, timedelta, UTC

from utils.news_sources import collect_all_mentions, fetch_kev_set, source_weight
from utils.priority_rules import classify_priority

DATA_DIR = os.path.join(os.path.dirname(__file__), "data")
os.makedirs(DATA_DIR, exist_ok=True)
OUT_PATH = os.path.join(DATA_DIR, "trending_cves.json")

HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36"}
NVD_BASE = "https://services.nvd.nist.gov/rest/json/cves/2.0"

MENTION_WINDOW_DAYS = 7
HALF_LIFE_DAYS = 2.0
RESULTS_LIMIT = 100
TOP_ONLY = True

def _decay_weight(age_days: float) -> float:
    return 0.5 ** (age_days / HALF_LIFE_DAYS)

def _fetch_nvd_one(cid: str) -> dict:
    try:
        r = requests.get(NVD_BASE, params={"cveId": cid}, headers=HEADERS, timeout=25)
        if not r.ok: return {}
        arr = r.json().get("vulnerabilities", [])
        if not arr: return {}
        cve = arr[0].get("cve", {})
        desc = ""
        try:
            ds = cve.get("descriptions") or []
            if ds: desc = ds[0].get("value") or ""
        except Exception: pass
        sev, score = "Medium", 0.0
        metrics = cve.get("metrics") or {}
        for m in ("cvssMetricV31","cvssMetricV30","cvssMetricV2"):
            mm = metrics.get(m) or []
            if mm:
                cv = mm[0].get("cvssData") or {}
                sev = cv.get("baseSeverity") or sev
                try: score = float(cv.get("baseScore") or 0.0)
                except Exception: pass
                break
        pub = cve.get("published") or cve.get("publishedDate")
        return {"description": desc, "severity": sev, "cvss_score": score, "published": pub}
    except Exception:
        return {}

def _fetch_epss_map(cve_ids: list[str]) -> dict:
    if not cve_ids: return {}
    out = {}
    base = "https://api.first.org/data/v1/epss"
    for i in range(0, len(cve_ids), 50):
        chunk = ",".join(cve_ids[i:i+50])
        try:
            r = requests.get(base, params={"cve": chunk}, headers=HEADERS, timeout=25)
            if r.ok:
                for row in r.json().get("data", []):
                    try: out[row["cve"].upper()] = float(row.get("epss") or 0.0)
                    except Exception: pass
        except Exception:
            pass
    return out

def _recent_nvd_fallback(window_days=7, limit=100) -> list[dict]:
    """If feeds fail, pull NVD last modified window and fabricate minimal 'events'."""
    now = datetime.now(UTC)
    start = (now - timedelta(days=window_days)).strftime("%Y-%m-%dT%H:%M:%S.000Z")
    end   = now.strftime("%Y-%m-%dT%H:%M:%S.000Z")

    events = []
    try:
        params = {"lastModStartDate": start, "lastModEndDate": end, "resultsPerPage": 2000, "startIndex": 0}
        while True:
            r = requests.get(NVD_BASE, params=params, headers=HEADERS, timeout=30)
            if not r.ok: break
            payload = r.json()
            vulns = payload.get("vulnerabilities", [])
            for v in vulns:
                c = v.get("cve", {})
                cid = (c.get("id") or "").upper()
                if not cid: continue
                # Use 'nvd' as a synthetic mention so scoring isn't zero
                events.append({"cve": cid, "source": "nvd", "dt": now, "flags": set()})
                if len(events) >= limit: break
            if len(events) >= limit: break
            total = payload.get("totalResults", 0)
            params["startIndex"] += payload.get("resultsPerPage", 2000)
            if params["startIndex"] >= total: break
    except Exception:
        pass
    return events

def run_scheduler():
    now = datetime.now(UTC)
    print("=== CVEPulse Trending + Criteria (verbose) ===")

    # 1) collect mentions from feeds
    raw = collect_all_mentions()
    print(f"[i] feed events total: {len(raw)}")
    cutoff = now - timedelta(days=MENTION_WINDOW_DAYS)
    events = [e for e in raw if e["dt"] and e["dt"] >= cutoff]
    print(f"[i] recent (<= {MENTION_WINDOW_DAYS}d): {len(events)}")

    if not events:
        print("[!] No recent feed events. Using NVD fallback window…")
        events = _recent_nvd_fallback(window_days=MENTION_WINDOW_DAYS, limit=RESULTS_LIMIT)
        print(f"[i] fallback NVD events: {len(events)}")

    kev = fetch_kev_set()
    print(f"[i] KEV size: {len(kev)}")

    # 2) score + signals
    per = {}
    for e in events:
        cid = e["cve"]; src = e["source"]; dt = e["dt"]
        age = (now - dt).total_seconds()/86400.0
        s = source_weight(src) * _decay_weight(age)
        rec = per.setdefault(cid, {"id": cid, "score": 0.0, "mentions": set(), "signals": set(), "last_seen": dt})
        rec["score"] += s
        rec["mentions"].add(src)
        rec["signals"] |= set(e.get("flags") or [])
        if dt > rec["last_seen"]: rec["last_seen"] = dt
        if cid in kev: rec["signals"].add("kev:list")

    ranked = sorted(per.values(), key=lambda x: (-x["score"], -x["last_seen"].timestamp()))
    top_ids = [r["id"] for r in ranked[:RESULTS_LIMIT]]
    print(f"[i] unique CVEs: {len(per)}; top considered: {len(top_ids)}")

    # 3) enrich: NVD + EPSS
    details = {cid: _fetch_nvd_one(cid) for cid in top_ids}
    epss_map = _fetch_epss_map(top_ids)

    # 4) build + classify
    results = []
    for r in ranked[:RESULTS_LIMIT]:
        cid = r["id"]
        info = details.get(cid, {})
        rec = {
            "id": cid,
            "description": info.get("description",""),
            "published": info.get("published"),
            "severity": info.get("severity","Medium"),
            "cvss_score": float(info.get("cvss_score") or 0.0),
            "mentions": sorted(list(r["mentions"])),
            "signals": sorted(list(r["signals"])),
            "epss": float(epss_map.get(cid, 0.0)),
            "sources": ["NVD"] + sorted(list(r["mentions"])),
            "trending_score": round(r["score"], 4),
            "last_seen": r["last_seen"].isoformat()
        }
        rec["priority"] = classify_priority(rec)
        results.append(rec)

    if TOP_ONLY:
        results = [x for x in results if x["priority"] in {"Emergency","Zero-Day","Critical","High"}]

    pr = {"Emergency":0,"Zero-Day":1,"Critical":2,"High":3,"Medium":4}
    results.sort(key=lambda x: (pr.get(x["priority"],9), -x["trending_score"], -(datetime.fromisoformat(x["last_seen"]).timestamp())))

    out = {"last_updated": now.isoformat(), "window_days": MENTION_WINDOW_DAYS, "results": results}
    with open(OUT_PATH, "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)

    print(f"[✓] Wrote {len(results)} → {OUT_PATH}")

if __name__ == "__main__":
    run_scheduler()
